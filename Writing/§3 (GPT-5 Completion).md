# §3 Notes, **by GPT-5** (not my writing but completing my argument)

A proof sketch of sorts: prediction markets work by aggregating _leaky_ knowledge — domains where each participant’s local signal spills partial truth into the public domain. Commodity supply, election polling, and sporting form are like this: there are many small, semi-independent clues, easily captured by prices. But the _future_ is rarely leaky in that way. Many of the outcomes we care about most depend on singular, path-dependent processes that have yet to be realised. No crowd can average its way to information that reality itself hasn’t generated.

Predictions about the future are therefore hard. The question becomes: **when are prediction markets actually useful?** The answer, I think, is when they do not merely _transmit_ existing information but **create** new information — when participation in the market incentivises genuine discovery. Hayek’s other great insight was that competition is a _discovery procedure_: entrepreneurs learn by trying things and being disciplined by profit and loss. Prediction markets succeed on the same principle when they induce traders to search, model, and test rather than simply express beliefs.

Financial markets exemplify this. Deep liquidity supports a research ecology: analysts build valuation models, gather alternative data, and refine priors. Pay-offs scale with model accuracy, so markets fund epistemic labour. Properly designed prediction markets could do likewise for public forecasting — subsidising participants who build better models of policy effects, macro variables, or scientific replication. The LMSR market-maker becomes, in this view, a **research grant allocator** whose payouts are proportional to ex-post correctness.

By contrast, an internal market at Google asking “Will Project Z ship by Q3?” draws effectively on dispersed worker knowledge but generates little _new_ knowledge. Employees have little incentive to spend hours formalising models, testing assumptions, or updating on fresh data when that effort competes with their salaried work. Here the Grossman–Stiglitz logic bites: if prices were already perfectly informative, nobody would expend effort to make them so. Information must be **paid for**, and in corporate settings the returns to further aggregation rarely justify the cost.

This brings us to the main point: **specialisation and the division of labour.** In most firms, dedicating teams to “forecast through markets” would simply replicate existing planning functions at higher overhead. The structural Coase–Stigler argument applies: use a market only when the expected gains from decentralised price discovery exceed the fixed costs of establishing and maintaining it. At small scales or in domains of limited ignorance, hierarchy and expert judgment suffice; at large scales, where knowledge creation itself is valuable, markets become worthwhile.

Thus, it is not that prediction markets are _ineffective_ in cases like Google’s internal trials — they aggregate what little dispersed knowledge exists quite well — but that there is **not much more value to extract**. Outsized returns appear only where markets do what no other institution will pay for: **the costly creation of new, testable knowledge about uncertain futures.**

The irony, then, is that the Hayekian miracle works best when society already has dense specialization and deep capital markets — precisely the contexts least in need of new epistemic machinery. Prediction markets are not underused because people are irrational or status-obsessed. They are underused because, outside of a few domains, they cannot sustain the division of cognitive labour that makes markets powerful in the first place.
## The Actually Good Objection

The best objection to prediction markets, then, is not moral, nor legal, nor even epistemic in the simple sense. It is structural. Prediction markets are tools for _aggregating existing knowledge_, not _creating new knowledge_ — and outside a few niches, most of the knowledge worth having must first be created.

Their ideal domain is one where:

1. There exists a large population with private, _already acquired_ signals about an uncertain but objectively resolvable future event.
    
2. The cost of expressing those signals (by trading) is low.
    
3. The benefit of aggregating them (a usable forecast) exceeds the cost of maintaining the market.
    
\
When any of these conditions fail — as they often do — prediction markets underperform, not because people are foolish, but because the epistemic division of labour has nowhere to attach.

---

## Epilogue: Where They Still Matter

If this diagnosis is right, it also suggests where prediction markets _can_ be transformative: in domains where they create incentives to build models and search for truth — that is, where they bootstrap the production of information rather than merely aggregate it.

Imagine a “forecasting industry” analogous to the early days of quantitative finance: traders, modelers, and AIs competing to refine probabilistic forecasts of major policy outcomes, climate trajectories, or biomedical breakthroughs, funded by governments or philanthropies who value epistemic accuracy as a public good. The resulting markets would not be parlor games; they would be collective research engines.

Until such institutions exist, prediction markets will remain brilliant curiosities — a proof-of-concept for how well we could know the future, if only we could afford to find it out.

---

#### **Footnotes**

1. Grossman, S., & Stiglitz, J. (1980). _On the Impossibility of Informationally Efficient Markets_. _American Economic Review_, 70(3), 393-408.
    
2. Keynes, J. M. (1937). _The General Theory of Employment_. _Quarterly Journal of Economics_, 51(2), 209–223.
    
3. Hayek, F. A. (1945). _The Use of Knowledge in Society_. _American Economic Review_, 35(4), 519–530.