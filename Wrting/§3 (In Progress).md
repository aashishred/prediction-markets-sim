## §3: The Actually Good Objection (Somewhat Contra Robin Hanson on Prediction Markets)

When your model diverges from reality, how do you decide which is wrong? The answer is usually obvious — until perhaps the object of study is humans, whereupon one feels the irresistible urge to declare that it is **they** who are mistaken. The reader of §2 is, I hope, now convinced that more people should be using prediction markets to satisfy their informational needs. And yet — [_Eppur si muove_](https://en.wikipedia.org/wiki/And_yet_it_moves) — they mostly do not. Why is this? We will try to avoid both the economist’s [standard move](https://en.wikipedia.org/wiki/Behavioral_economics) of declaring them irrational, and the [Hansonian move](https://www.amazon.co.uk/Elephant-Brain-Hidden-Motives-Everyday/dp/0197551955/) of re-describing them as rationally pursuing hidden motives. Instead, let us take their reluctance at face value, and ask what structural limitations of prediction markets themselves might explain their underuse[1](#footnote-1).

---
### I.
I’ll begin by confessing to some equivocation. In my exposition and defence of prediction markets, I made an argument inspired by Hayek’s famous 1945 article, [_The Use of Knowledge in Society_](https://www.kysq.org/docs/Hayek_45.pdf): that the strength of markets lies in their ability to incentivise the collection of information dispersed across individuals, and aggregate that information in price signals. Hayek was concerned with ordinary markets for goods and services, but my argument has been that prediction markets extend the same epistemic logic to beliefs about the future. But **what kind of information** would we need to aggregate, exactly, in order to usefully predict the future?

For Hayek, the price system allocates resources more effectively than any central planner could, by aggregating “**the knowledge of the particular circumstances of time and place**” — the local, often inarticulable, rapidly changing information about what people want (their subjective valuations); what resources and inputs are available where and in what condition; current production possibilities and bottlenecks; and the relevant opportunity costs. Prices transmit this diffused knowledge as signals of relative scarcity[2](#footnote-2), letting decentralised actors coordinate without anyone knowing the full underlying detail. Such tacit, practical knowledge is diffused across millions of individuals, and usually not written down — take the shopkeeper who knows what customers in their neighbourhood want this week — so a central planner cannot make use of it[3](#footnote-3), and thus cannot rival the adaptive efficiency of the market process.

Notice that the epistemic logic here depends on a specific ontology of knowledge, namely tacit, local, context-specific, and immediately actionable information about real resources and preferences. Crucially, it seems to presume that the relevant knowledge is **already present**, scattered across the minds of many individuals, waiting to be aggregated. The same logic underlies the Galtonian story from [§1](https://aashishreddy.substack.com/p/prediction-markets-introduction): the true weight of the ox had, in a sense, already leaked into the crowd — fragments of relevant knowledge were scattered among the villagers, converging in a remarkably accurate median estimate — and it simply needed to be extracted.

This invites the natural question: to what extent do anticipatory beliefs about the future have this structure? Or in other words, how leaky is the future to the present?

### II.
Consider as a motivating example the case of [Knightian uncertainty](https://en.wikipedia.org/wiki/Knightian_uncertainty), a situation in which the relevant outcomes and their probabilities are not merely unknown but indeterminate, so that no coherent ex ante probability distribution can be assigned. Keynes has beautifully explained the concept[4](#footnote-4)[5](#footnote-5):

> By “uncertain” knowledge, let me explain, I do not mean to distinguish what is known for certain from what is only probable. The game of roulette is not subject, in this sense, to uncertainty […] The sense in which I am using the term is that in which the prospect of a European war is uncertain, or the price of copper and the rate of interest twenty years hence, or the obsolescence of a new invention, or the position of private wealth-owners in the social system in 1970. **About these matters there is no scientific basis on which to form any calculable probability whatever. We simply do not know.** Nevertheless, the necessity for action and for decision compels us as practical men to do our best to overlook this awkward fact and to behave exactly as we should if we had behind us a good Benthamite calculation of a series of prospective advantages and disadvantages, each multiplied by its appropriate probability, waiting to be summed.

Evidently, if the information we are seeking is like one of Keynes’ examples, then although the probability implied by a market price may be “better than nothing”, the error bars around it are enormous. In such cases, the marginal gain in precision from aggregating many uncertain guesses may not justify the cost of elicitation; there’s just too little dispersed knowledge that the market might usefully aggregate.

Even if one does not fully accept this notion of radical Knightian uncertainty[6](#footnote-6), with its unquantifiable probabilities, the argument nonetheless suggests an important lemma: **it is often not that valuable to aggregate all information readily available to market participants**. If we want to estimate the likelihood a radically new project succeeds, the manager’s best guess may be less accurate than the crowd’s; but if both arise from the same basic uncertainty, the expected informational surplus of pooling their decentralised judgments is unlikely to justify the coordination costs involved.

### III.
I am not **just** making the boring and obvious Coasean argument that “[t]here is a cost of using the price mechanism” that might outweigh the informational benefits of prediction markets, leading organisations to prefer direct managerial judgement whenever it is cheaper[7](#footnote-7) — though this remains an important consideration, to which we will return. And I am certainly not making the boring and wrong argument that the future is so inscrutably uncertain that it’s pointless trying to reason probabilistically about it; that ignores the obvious success of financial markets in forecasting profits.

The real point is subtler. Recall that our original question was the extent to which the future leaks clues about its own development to the present. What we have seen is that in many domains, it fails to supply such clues to anyone’s mind; there is little dispersed private knowledge for prices to aggregate. So if prediction market prices only aggregate what is already known, in the way Hayek (1945) describes, their usefulness will be constrained by the poverty of the underlying knowledge set.
### Footnotes
[1](#footnote-anchor-1) Title cribbed from [Somewhat Contra Marcus On AI Scaling - by Scott Alexander](https://www.astralcodexten.com/p/somewhat-contra-marcus-on-ai-scaling)

[2](#footnote-anchor-2) As he writes:

> The marvel is that in a case like that of a scarcity of one raw material, without an order being issued, without more than perhaps a handful of people knowing the cause, tens of thousands of people whose identity could not be ascertained by months of investigation, are made to use the material or its products more sparingly; that is, they move in the right direction.

[3](#footnote-anchor-3) Since the central planner collects and has access only to statistics and “data”, and is therefore unable to make use of this local **knowledge**.

[4](#footnote-anchor-4) [John Maynard Keynes, “_General Theory of Employment_”, Quarterly Journal of Economics, 1937](https://www.jstor.org/stable/1882087); emphasis mine.

[5](#footnote-anchor-5) Frank Knight’s own description, in [_Risk, Uncertainty, and Profit_ (1921)](https://cdn.mises.org/Risk%2C%20Uncertainty%2C%20and%20Profit_4.pdf) is as follows:

> It will appear that a _measurable_ uncertainty, or “risk” proper, as we shall use the term, is so far different from an _unmeasurable_ one that it is not in effect an uncertainty at all. We shall accordingly restrict the term “uncertainty” to cases of the non-quantitive [sic] type. It is this “true” uncertainty, and not risk, as has been argued, which forms the basis of a valid theory of profit and accounts for the divergence between actual and theoretical competition.

[6](#footnote-anchor-6) My friend Ben Shindel, self-described “forecasting hobbyist” and author of Manifold’s weekly newsletter (!) was having none of it:

> I’ll quantify any uncertainty. […]
> 
> I don’t think there’s such a thing as truly “unquantifiable uncertainty” and I don’t think there’s a clear distinction between risk and uncertainty of the nature Knight proposes.

I agree with Ben that actually, some of Keynes’ examples are pretty bad; as he says:

> [T]he prospect of a European war is not fundamentally different from a roulette wheel. It’s just far more complex. […] There are also tons of “scientific bases” upon which to forecast. Buildup of armaments. Population growth. Number of border incidents over time. Etc etc.

I still think that Knightian uncertainty is a real phenomenon, and that the distinction between risk and uncertainty is a legitimate one, for the following kind of reason: 

> What if I phrased it in terms of uncertainty re the space of outcomes to which we’re assigning probabilities? So obviously probabilities require some kind of event space, and you’re saying clearly there will always be better and worse probability assignments. […]
> 
> But the point is that (say, scientific) experimentation can reveal outcomes (elements of the sample space) which we didn’t know existed. And so there’s a kind of irreducible uncertainty in that we can’t assign ex ante probabilities to *those* outcomes.

I think I need to figure out some philosophy to be really confident here, but as I say, this is more of an intuition pump than really crucial to my ultimate argument. If you’re interested, you can see our full conversation [here](https://x.com/_AashishReddy/status/1958513649485553766?s=20). You should also subscribe to [Ben’s Substack](https://thebsdetector.substack.com/).

[7](#footnote-anchor-7) From [_The Nature of the Firm_ (1937)](https://www.jstor.org/stable/2626876?seq=1):

> “The main reason why it is profitable to establish a firm would seem to be that there is a cost of using the price mechanism.”  
> — _Section II_

> “The costs of carrying out a transaction by means of an exchange on the open market must be less than the costs of organizing the same transaction within the firm.”  
> — _Section IV_

> “A firm will tend to expand until the costs of organizing an extra transaction within the firm become equal to the costs of carrying out the same transaction by means of an exchange on the open market.”  
> — _Section IV_